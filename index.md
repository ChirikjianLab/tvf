# Transporters with Visual Foresight

Rearrangement tasks have been identified as a crucial challenge for intelligent robotic manipulation, but few methods allow for precise construction of unseen structures. We propose a visual foresight model for pick-and-place manipulation which is able to learn efficiently. In addition, we develop a multi-modal action proposal module which builds on Goal-Conditioned Transporter Networks, a state-of-the-art imitation learning method. Our method, Transporters with Visual Foresight (TVF), enables task planning from image data and is able to achieve multi-task learning and zero-shot generalization to unseen tasks with only a handful of expert demonstrations. TVF is able to improve the performance of a state-of-the-art imitation learning method on both training and unseen tasks in simulation and real robot experiments. In particular, the average success rate on unseen tasks improves from 55.0% to 77.9% in simulation experiments and from 30% to 63.3% in real robot experiments when given only tens of expert demonstrations.

# Authors
[Hongtao Wu\*](https://hongtaowu67.github.io/), Jikai Ye\*, Xin Meng, [Chris Paxton](https://cpaxton.github.io/about/), [Gregory Chirikjian](https://cde.nus.edu.sg/me/staff/chirikjian-gregory-s/)

\* indicates equal contributions.

Laboratory for Computational and Sensing Robotics (LCSR), Johns Hopkins University

Department of Mechanical Engineering, National University of Singapore

NVIDIA

# Links
- [Arxiv](https://arxiv.org/abs/2202.10765)
- [Supplementary Material](./supplementary.pdf)
- [Introductory Video](https://youtu.be/XmXSKsFTcgI)

# Introductory Video
<iframe width="640" height="360" src="https://www.youtube.com/embed/XmXSKsFTcgI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
